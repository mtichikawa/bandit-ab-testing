{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit A/B Testing - Complete Analysis\n",
    "\n",
    "This notebook demonstrates adaptive A/B testing using multi-armed bandit algorithms. We compare Thompson Sampling, UCB1, and Epsilon-Greedy against traditional fixed-split A/B testing across a realistic e-commerce conversion rate optimization scenario.\n",
    "\n",
    "**Key Questions:**\n",
    "- How quickly do bandit algorithms identify the best variant?\n",
    "- How much regret (lost conversions) does each algorithm accumulate?\n",
    "- When should you use bandits vs. traditional A/B testing?\n",
    "\n",
    "**Scenario:** Testing 3 homepage CTA button variants with true conversion rates of 3%, 5%, and 4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "print('Libraries loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bandit Algorithm Implementations\n",
    "\n",
    "We implement the three core algorithms inline for notebook clarity. These mirror the production implementations in `bandits/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling:\n",
    "    \"\"\"Beta-Bernoulli Thompson Sampling.\n",
    "    \n",
    "    Maintains Beta(alpha, beta) posterior for each arm's conversion rate.\n",
    "    Samples from posteriors and picks the arm with the highest sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.alpha = np.ones(n_arms)  # successes + 1\n",
    "        self.beta = np.ones(n_arms)   # failures + 1\n",
    "        self.pulls = np.zeros(n_arms)\n",
    "        self.rewards = np.zeros(n_arms)\n",
    "        self.history = []  # track arm selections\n",
    "\n",
    "    def select_arm(self):\n",
    "        samples = np.random.beta(self.alpha, self.beta)\n",
    "        return int(np.argmax(samples))\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.pulls[arm] += 1\n",
    "        self.rewards[arm] += reward\n",
    "        self.alpha[arm] += reward\n",
    "        self.beta[arm] += (1 - reward)\n",
    "        self.history.append(arm)\n",
    "\n",
    "    def get_win_probability(self, n_samples=10000):\n",
    "        samples = np.random.beta(\n",
    "            self.alpha[:, np.newaxis],\n",
    "            self.beta[:, np.newaxis],\n",
    "            size=(self.n_arms, n_samples)\n",
    "        )\n",
    "        best_arm = np.argmax(samples, axis=0)\n",
    "        return np.bincount(best_arm, minlength=self.n_arms) / n_samples\n",
    "\n",
    "\n",
    "class UCB1:\n",
    "    \"\"\"Upper Confidence Bound 1 algorithm.\n",
    "    \n",
    "    Selects arms based on optimistic estimates: mean reward + exploration bonus.\n",
    "    Bonus shrinks as arm is pulled more, balancing exploration/exploitation.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.pulls = np.zeros(n_arms)\n",
    "        self.rewards = np.zeros(n_arms)\n",
    "        self.total_pulls = 0\n",
    "        self.history = []\n",
    "\n",
    "    def select_arm(self):\n",
    "        if self.total_pulls < self.n_arms:\n",
    "            return self.total_pulls\n",
    "        means = self.rewards / np.maximum(self.pulls, 1)\n",
    "        bonus = np.sqrt(2 * np.log(self.total_pulls) / np.maximum(self.pulls, 1))\n",
    "        return int(np.argmax(means + bonus))\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.pulls[arm] += 1\n",
    "        self.rewards[arm] += reward\n",
    "        self.total_pulls += 1\n",
    "        self.history.append(arm)\n",
    "\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    \"\"\"Epsilon-Greedy baseline.\"\"\"\n",
    "    def __init__(self, n_arms, epsilon=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.pulls = np.zeros(n_arms)\n",
    "        self.rewards = np.zeros(n_arms)\n",
    "        self.history = []\n",
    "\n",
    "    def select_arm(self):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)\n",
    "        means = self.rewards / np.maximum(self.pulls, 1)\n",
    "        return int(np.argmax(means))\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.pulls[arm] += 1\n",
    "        self.rewards[arm] += reward\n",
    "        self.history.append(arm)\n",
    "\n",
    "\n",
    "print('Bandit classes defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulation Setup\n",
    "\n",
    "**Scenario:** E-commerce homepage A/B/C test\n",
    "- **Variant A** (control): Blue button — 3% conversion rate\n",
    "- **Variant B**: Green button — 5% conversion rate (true winner)\n",
    "- **Variant C**: Red button — 4% conversion rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "TRUE_RATES = [0.03, 0.05, 0.04]  # true conversion rates\n",
    "VARIANT_NAMES = ['Variant A (Blue, 3%)', 'Variant B (Green, 5%)', 'Variant C (Red, 4%)']\n",
    "N_TRIALS = 5000\n",
    "N_SIMULATIONS = 200  # for averaging across random seeds\n",
    "\n",
    "BEST_ARM = np.argmax(TRUE_RATES)  # arm 1\n",
    "BEST_RATE = TRUE_RATES[BEST_ARM]\n",
    "\n",
    "print(f'True conversion rates: {[f\"{r:.0%}\" for r in TRUE_RATES]}')\n",
    "print(f'Best variant: {VARIANT_NAMES[BEST_ARM]}')\n",
    "print(f'Optimal total conversions in {N_TRIALS} trials: {BEST_RATE * N_TRIALS:.0f}')\n",
    "\n",
    "COLORS = ['#e74c3c', '#2ecc71', '#3498db', '#9b59b6']\n",
    "ALGO_NAMES = ['Thompson Sampling', 'UCB1', 'Epsilon-Greedy (ε=0.1)', 'Traditional A/B (33/33/33)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(algorithm_class, true_rates, n_trials, **kwargs):\n",
    "    \"\"\"Run a single simulation and return per-trial tracking data.\"\"\"\n",
    "    bandit = algorithm_class(len(true_rates), **kwargs)\n",
    "    \n",
    "    cumulative_reward = []\n",
    "    cumulative_regret = []\n",
    "    arm_selections = []\n",
    "    total_reward = 0\n",
    "    total_regret = 0\n",
    "    \n",
    "    for t in range(n_trials):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = 1 if np.random.random() < true_rates[arm] else 0\n",
    "        bandit.update(arm, reward)\n",
    "        \n",
    "        total_reward += reward\n",
    "        total_regret += BEST_RATE - true_rates[arm]\n",
    "        cumulative_reward.append(total_reward)\n",
    "        cumulative_regret.append(total_regret)\n",
    "        arm_selections.append(arm)\n",
    "    \n",
    "    return {\n",
    "        'bandit': bandit,\n",
    "        'cumulative_reward': np.array(cumulative_reward),\n",
    "        'cumulative_regret': np.array(cumulative_regret),\n",
    "        'arm_selections': np.array(arm_selections),\n",
    "        'pulls': bandit.pulls,\n",
    "        'rewards': bandit.rewards,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_traditional_ab(true_rates, n_trials):\n",
    "    \"\"\"Simulate traditional equal-split A/B/C test.\"\"\"\n",
    "    n_arms = len(true_rates)\n",
    "    pulls = np.zeros(n_arms)\n",
    "    rewards = np.zeros(n_arms)\n",
    "    cumulative_reward = []\n",
    "    cumulative_regret = []\n",
    "    arm_selections = []\n",
    "    total_reward = 0\n",
    "    total_regret = 0\n",
    "    \n",
    "    for t in range(n_trials):\n",
    "        arm = t % n_arms  # round-robin equal split\n",
    "        reward = 1 if np.random.random() < true_rates[arm] else 0\n",
    "        pulls[arm] += 1\n",
    "        rewards[arm] += reward\n",
    "        total_reward += reward\n",
    "        total_regret += BEST_RATE - true_rates[arm]\n",
    "        cumulative_reward.append(total_reward)\n",
    "        cumulative_regret.append(total_regret)\n",
    "        arm_selections.append(arm)\n",
    "    \n",
    "    return {\n",
    "        'cumulative_reward': np.array(cumulative_reward),\n",
    "        'cumulative_regret': np.array(cumulative_regret),\n",
    "        'arm_selections': np.array(arm_selections),\n",
    "        'pulls': pulls,\n",
    "        'rewards': rewards,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run single simulations for visualization\n",
    "np.random.seed(42)\n",
    "results_ts = run_simulation(ThompsonSampling, TRUE_RATES, N_TRIALS)\n",
    "results_ucb = run_simulation(UCB1, TRUE_RATES, N_TRIALS)\n",
    "results_eg = run_simulation(EpsilonGreedy, TRUE_RATES, N_TRIALS, epsilon=0.1)\n",
    "results_ab = run_traditional_ab(TRUE_RATES, N_TRIALS)\n",
    "\n",
    "all_results = [results_ts, results_ucb, results_eg, results_ab]\n",
    "\n",
    "print('Simulations complete')\n",
    "print(f'\\nFinal arm distribution after {N_TRIALS} trials:')\n",
    "for name, res in zip(ALGO_NAMES, all_results):\n",
    "    pct = res['pulls'] / res['pulls'].sum() * 100\n",
    "    print(f'  {name}: A={pct[0]:.1f}%  B={pct[1]:.1f}%  C={pct[2]:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cumulative Regret Comparison\n",
    "\n",
    "Regret measures the total conversions lost by not always choosing the best arm. Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "trials = np.arange(1, N_TRIALS + 1)\n",
    "\n",
    "# Cumulative regret\n",
    "ax = axes[0]\n",
    "for res, name, color in zip(all_results, ALGO_NAMES, COLORS):\n",
    "    ax.plot(trials, res['cumulative_regret'], label=name, color=color, linewidth=2)\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('Cumulative Regret (lost conversions)')\n",
    "ax.set_title('Cumulative Regret Over Time')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Cumulative reward\n",
    "ax = axes[1]\n",
    "for res, name, color in zip(all_results, ALGO_NAMES, COLORS):\n",
    "    ax.plot(trials, res['cumulative_reward'], label=name, color=color, linewidth=2)\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('Cumulative Conversions')\n",
    "ax.set_title('Cumulative Conversions Over Time')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/regret_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nFinal regret summary:')\n",
    "for name, res in zip(ALGO_NAMES, all_results):\n",
    "    print(f'  {name}: {res[\"cumulative_regret\"][-1]:.1f} regret, {res[\"cumulative_reward\"][-1]:.0f} conversions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Beta Distribution Evolution — Thompson Sampling\n",
    "\n",
    "Visualizing how the posterior beliefs about each arm's conversion rate evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = [50, 200, 500, 2000, 5000]\n",
    "x = np.linspace(0, 0.15, 1000)\n",
    "arm_colors = ['#e74c3c', '#2ecc71', '#3498db']\n",
    "arm_labels = ['Variant A (3%)', 'Variant B (5%)', 'Variant C (4%)']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(checkpoints), figsize=(18, 4), sharey=False)\n",
    "\n",
    "# Re-run TS and snapshot Beta params at each checkpoint\n",
    "np.random.seed(42)\n",
    "bandit_snap = ThompsonSampling(3)\n",
    "checkpoint_states = []\n",
    "cp_idx = 0\n",
    "\n",
    "for t in range(N_TRIALS):\n",
    "    if cp_idx < len(checkpoints) and t == checkpoints[cp_idx] - 1:\n",
    "        checkpoint_states.append((bandit_snap.alpha.copy(), bandit_snap.beta.copy()))\n",
    "        cp_idx += 1\n",
    "    arm = bandit_snap.select_arm()\n",
    "    reward = 1 if np.random.random() < TRUE_RATES[arm] else 0\n",
    "    bandit_snap.update(arm, reward)\n",
    "\n",
    "# Add final state if not captured\n",
    "if len(checkpoint_states) < len(checkpoints):\n",
    "    checkpoint_states.append((bandit_snap.alpha.copy(), bandit_snap.beta.copy()))\n",
    "\n",
    "for i, (ax, cp, (alpha, beta)) in enumerate(zip(axes, checkpoints, checkpoint_states)):\n",
    "    for arm in range(3):\n",
    "        pdf = stats.beta.pdf(x, alpha[arm], beta[arm])\n",
    "        ax.plot(x, pdf, color=arm_colors[arm], linewidth=2, label=arm_labels[arm])\n",
    "        ax.fill_between(x, pdf, alpha=0.15, color=arm_colors[arm])\n",
    "    \n",
    "    # True rate markers\n",
    "    for arm, rate in enumerate(TRUE_RATES):\n",
    "        ax.axvline(rate, color=arm_colors[arm], linestyle='--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    ax.set_title(f'Trial {cp:,}', fontweight='bold')\n",
    "    ax.set_xlabel('Conversion Rate')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Probability Density')\n",
    "        ax.legend(fontsize=8)\n",
    "    ax.set_xlim(0, 0.12)\n",
    "\n",
    "fig.suptitle('Beta Posterior Evolution — Thompson Sampling\\n(dashed lines = true rates)', \n",
    "             fontsize=12, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/beta_evolution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('As trials increase, posteriors narrow and concentrate around true rates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Arm Selection Over Time\n",
    "\n",
    "How quickly does each algorithm converge to always pulling the best arm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 200  # rolling window for smoothing\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, res, name, color in zip(axes, all_results, ALGO_NAMES, COLORS):\n",
    "    selections = res['arm_selections']\n",
    "    \n",
    "    for arm_idx, arm_color, arm_label in zip(range(3), arm_colors, arm_labels):\n",
    "        is_arm = (selections == arm_idx).astype(float)\n",
    "        rolling_pct = pd.Series(is_arm).rolling(window, min_periods=1).mean() * 100\n",
    "        ax.plot(rolling_pct, color=arm_color, linewidth=1.5, label=arm_label, alpha=0.9)\n",
    "    \n",
    "    ax.axhline(100/3, color='gray', linestyle=':', alpha=0.5, label='Equal split (33%)')\n",
    "    ax.set_title(name, fontweight='bold')\n",
    "    ax.set_xlabel('Trial')\n",
    "    ax.set_ylabel(f'% Traffic ({window}-trial rolling avg)')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "plt.suptitle('Traffic Allocation Over Time — Convergence to Best Variant', \n",
    "             fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/arm_selection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monte Carlo Analysis — Average Performance Over 200 Simulations\n",
    "\n",
    "Single simulations have high variance. We average over 200 runs to get stable estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Running {N_SIMULATIONS} simulations per algorithm...')\n",
    "\n",
    "mc_regret = {name: [] for name in ALGO_NAMES}\n",
    "mc_reward = {name: [] for name in ALGO_NAMES}\n",
    "\n",
    "for sim in range(N_SIMULATIONS):\n",
    "    np.random.seed(sim)\n",
    "    r_ts = run_simulation(ThompsonSampling, TRUE_RATES, N_TRIALS)\n",
    "    r_ucb = run_simulation(UCB1, TRUE_RATES, N_TRIALS)\n",
    "    r_eg = run_simulation(EpsilonGreedy, TRUE_RATES, N_TRIALS, epsilon=0.1)\n",
    "    r_ab = run_traditional_ab(TRUE_RATES, N_TRIALS)\n",
    "    \n",
    "    for name, res in zip(ALGO_NAMES, [r_ts, r_ucb, r_eg, r_ab]):\n",
    "        mc_regret[name].append(res['cumulative_regret'])\n",
    "        mc_reward[name].append(res['cumulative_reward'])\n",
    "\n",
    "# Convert to arrays\n",
    "for name in ALGO_NAMES:\n",
    "    mc_regret[name] = np.array(mc_regret[name])\n",
    "    mc_reward[name] = np.array(mc_reward[name])\n",
    "\n",
    "print('Monte Carlo complete')\n",
    "\n",
    "# Plot mean regret with confidence bands\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, metric, title in zip(axes, [mc_regret, mc_reward], ['Cumulative Regret', 'Cumulative Conversions']):\n",
    "    for name, color in zip(ALGO_NAMES, COLORS):\n",
    "        mean = metric[name].mean(axis=0)\n",
    "        std = metric[name].std(axis=0)\n",
    "        ax.plot(trials, mean, label=name, color=color, linewidth=2)\n",
    "        ax.fill_between(trials, mean - std, mean + std, alpha=0.15, color=color)\n",
    "    ax.set_xlabel('Trial')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_title(f'{title} (mean ± 1σ, n={N_SIMULATIONS} sims)')\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/monte_carlo.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Summary & Business Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 65)\n",
    "print(f'SIMULATION RESULTS — {N_TRIALS:,} trials, {N_SIMULATIONS} Monte Carlo runs')\n",
    "print('=' * 65)\n",
    "\n",
    "summary_rows = []\n",
    "for name in ALGO_NAMES:\n",
    "    final_regret = mc_regret[name][:, -1]\n",
    "    final_reward = mc_reward[name][:, -1]\n",
    "    optimal = BEST_RATE * N_TRIALS\n",
    "    \n",
    "    row = {\n",
    "        'Algorithm': name,\n",
    "        'Avg Conversions': f'{final_reward.mean():.0f}',\n",
    "        'Avg Regret': f'{final_regret.mean():.1f}',\n",
    "        'Regret %': f'{(final_regret.mean() / optimal * 100):.1f}%',\n",
    "        'vs Traditional': '',\n",
    "    }\n",
    "    summary_rows.append((name, final_reward.mean(), final_regret.mean()))\n",
    "\n",
    "trad_reward = summary_rows[-1][1]\n",
    "\n",
    "print(f'\\n{\"Algorithm\":<35} {\"Avg Conv\":>10} {\"Avg Regret\":>12} {\"vs Traditional\":>15}')\n",
    "print('-' * 75)\n",
    "for name, reward, regret in summary_rows:\n",
    "    lift = reward - trad_reward\n",
    "    lift_pct = (lift / trad_reward) * 100\n",
    "    flag = ' ← best' if lift == max(r - trad_reward for _, r, _ in summary_rows) else ''\n",
    "    print(f'{name:<35} {reward:>10.0f} {regret:>12.1f} {lift_pct:>+14.1f}%{flag}')\n",
    "\n",
    "print('\\n')\n",
    "print('Business interpretation:')\n",
    "best_algo = max(summary_rows[:-1], key=lambda x: x[1])\n",
    "lift_abs = best_algo[1] - trad_reward\n",
    "print(f'  Best algorithm: {best_algo[0]}')\n",
    "print(f'  Additional conversions vs traditional A/B: +{lift_abs:.0f} per {N_TRIALS:,} visitors')\n",
    "print(f'  At $50 avg order value: +${lift_abs * 50:,.0f} revenue per {N_TRIALS:,} visitors')\n",
    "print(f'  At 100k monthly visitors: +${lift_abs * 50 * 20:,.0f}/month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Win Probability — Thompson Sampling\n",
    "\n",
    "A key advantage of Thompson Sampling: we always have a probabilistic estimate of which variant is winning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track win probabilities at intervals\n",
    "np.random.seed(42)\n",
    "bandit_wp = ThompsonSampling(3)\n",
    "wp_checkpoints = list(range(0, N_TRIALS + 1, 100))\n",
    "win_probs = {arm: [] for arm in range(3)}\n",
    "\n",
    "for t in range(N_TRIALS):\n",
    "    if t in wp_checkpoints:\n",
    "        wp = bandit_wp.get_win_probability()\n",
    "        for arm in range(3):\n",
    "            win_probs[arm].append(wp[arm])\n",
    "    arm = bandit_wp.select_arm()\n",
    "    reward = 1 if np.random.random() < TRUE_RATES[arm] else 0\n",
    "    bandit_wp.update(arm, reward)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "for arm in range(3):\n",
    "    ax.plot(wp_checkpoints[:len(win_probs[arm])], \n",
    "            [p * 100 for p in win_probs[arm]], \n",
    "            color=arm_colors[arm], linewidth=2.5, label=arm_labels[arm])\n",
    "\n",
    "ax.axhline(95, color='gray', linestyle='--', alpha=0.5, label='95% confidence threshold')\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('Probability of Being Best Variant (%)')\n",
    "ax.set_title('Thompson Sampling — Win Probability Over Time', fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_ylim(0, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/win_probability.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "final_wp = bandit_wp.get_win_probability()\n",
    "print('Final win probabilities:')\n",
    "for arm, (name, wp) in enumerate(zip(arm_labels, final_wp)):\n",
    "    print(f'  {name}: {wp:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "**Key findings:**\n",
    "\n",
    "1. **Thompson Sampling** achieves the lowest regret and highest conversions by efficiently routing traffic to the best variant while maintaining exploration.\n",
    "\n",
    "2. **UCB1** is competitive and deterministic — useful when reproducibility matters more than absolute optimality.\n",
    "\n",
    "3. **Epsilon-Greedy** (ε=0.1) is simple but wasteful — 10% of traffic is always random regardless of evidence.\n",
    "\n",
    "4. **Traditional A/B** accumulates the most regret because it never adapts — it keeps sending equal traffic to known inferior variants until the test ends.\n",
    "\n",
    "**When to use each:**\n",
    "- **Thompson Sampling**: Revenue-sensitive tests where every visit has monetary value (e-commerce, ads)\n",
    "- **UCB1**: Academic/regulated environments requiring deterministic decisions\n",
    "- **Traditional A/B**: Strict statistical inference requirements, regulatory contexts\n",
    "\n",
    "**Production considerations:** The `api/` directory contains a FastAPI service wrapping these algorithms with Redis state persistence, allowing real-time variant selection at scale. See `README.md` for deployment instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Notebook complete.')\n",
    "print('Generated plots saved to ../docs/')\n",
    "print('\\nFiles produced:')\n",
    "for f in ['regret_comparison.png', 'beta_evolution.png', 'arm_selection.png', \n",
    "          'monte_carlo.png', 'win_probability.png']:\n",
    "    print(f'  docs/{f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
